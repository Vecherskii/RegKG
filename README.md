````markdown
# Regulatory Knowledge Graph Pipeline (reg_kg) üöÄ

## 1. Project Vision

This project's mission is to build a **multimodal Knowledge Graph (KG)** from financial regulatory documents (e.g., from BaFin, ECB, Basel).

We aim to create a "digital twin" of the regulatory landscape. Instead of just modeling the text, our graph will also incorporate and describe the charts, tables, and formulas within these documents. This will allow us to ask complex questions and find not just relevant text snippets, but the *exact artifacts* (visuals, data, calculations) that provide evidence for an answer.

Our ultimate goal is to build a **GraphRAG** (Graph Retrieval-Augmented Generation) system. This advanced technique uses the structured knowledge in our KG to improve the accuracy and relevance of Large Language Models (LLMs). It will allow us to ask natural language questions and get answers that are "grounded" in the verified facts and relationships from our graph, reducing hallucinations and providing traceable, verifiable responses.

---

## 2. Quick Start (How to Develop)

Follow these steps to set up your development environment:

1.  **Clone the Repo:** Get the project code onto your machine (or Azure environment).
    ```bash
    git clone [your-repo-url]
    cd reg_kg
    ```
    * **What is Git?** Git is the industry standard for *version control*. It lets us track changes to our code, collaborate without conflicts, and revert mistakes easily. Think of it as "track changes" for code, but much more powerful.

2.  **Create the Environment:** We use **Conda** for managing Python packages and environments.
    ```bash
    conda env create -f environment.yml
    conda activate reg_kg_env
    ```
    * **What is Conda?** Conda is a package and environment manager. It creates isolated spaces for projects, ensuring that the libraries needed for this project don't conflict with others on your system. It's particularly good at handling complex scientific libraries (like the ones we use for NLP and PDF parsing) that might have non-Python dependencies. `environment.yml` lists all the packages needed.

3.  **Run the Pipeline:** We use **Typer** to create a clean command-line interface (CLI) for running our pipeline steps. All commands are run from the `src/main.py` script.
    ```bash
    # See all available commands and their descriptions
    python src/main.py --help

    # Test if the environment and CLI are working correctly
    python src/main.py hello "YourName"

    # Run a specific pipeline step (e.g., Epic 1 - this is a placeholder for now)
    python src/main.py run-epic-1
    ```
    * **What is Typer?** Typer helps build professional CLIs easily. Instead of needing to manually edit `main.py` to run different parts of our code, Typer lets us define simple commands (like `run-epic-1`), making the pipeline easy to operate and test.

---

## 3. Project Structure (The "Project Tour") üó∫Ô∏è

This project is structured as a modular pipeline, like an assembly line. Each "Epic" represents a stage in this line. The folder structure is designed to keep things organized:

* **`/reg_kg`** (Project Root)
    * **`.git/`** (Git version control files, usually hidden)
    * **`.gitignore`** (Tells Git which files to ignore, like environment files)
    * **`data/`** (All project data lives here. **NEVER** commit large data files to Git.)
        * **`raw_pdfs/`** (INPUT: The original regulatory PDF documents.)
        * **`processed/`** (INTERMEDIATE: Cleaned, structured data like JSONL files generated by pipeline steps.)
        * **`artifacts/`** (INTERMEDIATE: Images and tables extracted from PDFs.)
    * **`ontology/`** (The "Blueprints" for our Knowledge Graph.)
        * **`(fibo.owl)`** (The FIBO ontology files will be stored here.)
    * **`src/`** (ALL our Python source code. This is where we build the pipeline.)
        * **`__init__.py`** (Makes 'src' a Python package.)
        * **`main.py`** (The main "Control Panel" script using Typer. **You run this!**)
        * **`pipeline/`** (Core pipeline logic. Each major step ("Epic") gets its own Python file here.)
            * **`__init__.py`**
            * **`epic_1_parser.py`** (Code for parsing PDFs - Epic 1.)
            * **`... (other epics)`**
        * **`utils/`** (Reusable helper code, e.g., logging setup, database connections.)
    * **`notebooks/`** (Jupyter notebooks for exploration, analysis, and experiments. Code here should be considered temporary or for demos, not part of the final pipeline.)
    * **`tests/`** (Automated tests using pytest to ensure our pipeline code works correctly.)
    * **`environment.yml`** (The Conda environment definition file.)
    * **`README.md`** (This file! Your guide to the project.)

---

## 4. Key Libraries (What They Are & Why We Use Them)

This project relies on a few powerful, specialized libraries. Understanding their role is key:

* **`PyMuPDF` (imported as `fitz`)**:
    * **What:** A high-performance Python library for accessing PDF documents.
    * **Why (Epic 1 & 7):** It's extremely fast and accurate for extracting raw text, layout information (like text coordinates, which help us identify structure like headers and paragraphs), and embedded images. It's the foundation of our document ingestion.

* **`spaCy`**:
    * **What:** A leading library for advanced Natural Language Processing (NLP) in Python.
    * **Why (Epic 4 & 5):** This is our "language brain." We'll use it to process the extracted text, identify key financial entities (Named Entity Recognition - NER), understand their relationships (Relation Extraction - RE), and link them to their base forms (Lemmatization). It's designed for building robust, production-ready NLP pipelines.

* **`rdflib`**:
    * **What:** The primary Python library for working with RDF (Resource Description Framework), the data format used for Knowledge Graphs.
    * **Why (Epic 6 & 9):** This library allows us to create and manipulate the `(subject, predicate, object)` triples that form our Knowledge Graph. It lets us load ontologies (like FIBO) and add our extracted data, effectively building the graph in memory or saving it to files.

* **`pandas`**:
    * **What:** The go-to library in Python for data manipulation and analysis, especially tabular data.
    * **Why (Epic 7 & 8):** We'll use it primarily to handle the tables extracted from PDFs (using libraries like Camelot, potentially added later) and to easily convert them into textual descriptions for our graph.

* **`Typer`**: (Already explained in Quick Start)
    * **Why:** Provides the user-friendly command-line interface to orchestrate our pipeline.

* **`JupyterLab`**:
    * **What:** An interactive development environment, popular in data science.
    * **Why:** Used in the `/notebooks` directory for experimentation, visualization, and trying out ideas quickly before adding them to the main pipeline code in `/src`.

* **`pytest`**:
    * **What:** A framework for writing and running automated tests for our Python code.
    * **Why:** Ensures that as we build and modify our pipeline, we don't accidentally break existing parts. Crucial for building reliable software.

---

### Definition of Done (Checklist)

You can confirm this Epic is finished when:
* [ ] A new Git repository is created and the `README.md` content above is committed.
* [ ] The `environment.yml` file is in the root.
* [ ] A new team member can clone the repo and **successfully** create the environment with `conda env create -f environment.yml`.
* [ ] The new member can run `conda activate reg_kg_env` and **successfully** run `python src/main.py hello "Test"` and see the success message.
* [ ] The complete folder structure (including the empty `.gitkeep` files for empty directories) is committed to the `main` branch.
* [ ] The `.gitignore` file includes common Python ignores (`__pycache__/`, `*.pyc`, `*.env`) and environment-specific ignores (`/reg_kg_env/` if created locally).
````
